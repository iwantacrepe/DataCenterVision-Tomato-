{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb079fac",
   "metadata": {},
   "source": [
    "# Multilabel Tomato Disease Classification\n",
    "This notebook trains ResNet50, EfficientNet-B0, and DenseNet121 to classify tomato leaf images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c53709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to your CSV\n",
    "csv_path = 'Variant-b(MultiLabel Classification)/Multi-Label dataset - with augmented.csv'\n",
    "\n",
    "# read CSV, only keep dataset & filename + label columns\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# drop the 'path' column since we'll construct it from dataset+filename\n",
    "df = df.drop(columns=['path', 'SUM'], errors='ignore')\n",
    "\n",
    "# fill NA with 0 and ensure numeric\n",
    "label_cols = df.columns.drop(['dataset','filename'])\n",
    "df[label_cols] = df[label_cols].fillna(0).astype(int)\n",
    "\n",
    "# preview\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1: Class distribution (how many images per label)\n",
    "plt.figure(figsize=(10,4))\n",
    "df[label_cols].sum().sort_values().plot.barh()\n",
    "plt.title(\"Label frequency over entire dataset\")\n",
    "plt.xlabel(\"Number of images\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.2: Co-occurrence heatmap\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "corr = df[label_cols].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Label co-occurrence correlation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TomatoDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, split, transform=None):\n",
    "        \"\"\"\n",
    "        df         : DataFrame with columns ['dataset','filename',...labels]\n",
    "        root_dir   : e.g. 'Variant-b(MultiLabel Classification)'\n",
    "        split      : one of 'train','val','test'\n",
    "        transform  : torchvision transforms\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.data = df[df['dataset'] == split].reset_index(drop=True)\n",
    "        self.root = os.path.join(root_dir, split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.root, row['filename'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        labels = torch.FloatTensor(row[label_cols].values)\n",
    "        return img, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748414e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "train_tfms = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "val_tfms = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# create datasets\n",
    "root = 'Variant-b(MultiLabel Classification)'\n",
    "train_ds = TomatoDataset(df, root, 'train', transform=train_tfms)\n",
    "val_ds   = TomatoDataset(df, root, 'val',   transform=val_tfms)\n",
    "test_ds  = TomatoDataset(df, root, 'test',  transform=val_tfms)\n",
    "\n",
    "# loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b49490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(name, num_labels):\n",
    "    \"\"\"\n",
    "    Return a pretrained backbone with a final sigmoid layer for multi-label.\n",
    "    name: one of 'resnet50', 'densenet121', 'efficientnet_b0'\n",
    "    \"\"\"\n",
    "    if name == 'resnet50':\n",
    "        m = models.resnet50(pretrained=True)\n",
    "        in_f = m.fc.in_features\n",
    "        m.fc = nn.Linear(in_f, num_labels)\n",
    "    elif name == 'densenet121':\n",
    "        m = models.densenet121(pretrained=True)\n",
    "        in_f = m.classifier.in_features\n",
    "        m.classifier = nn.Linear(in_f, num_labels)\n",
    "    elif name == 'efficientnet_b0':\n",
    "        m = models.efficientnet_b0(pretrained=True)\n",
    "        in_f = m.classifier[1].in_features\n",
    "        m.classifier[1] = nn.Linear(in_f, num_labels)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    return m\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, opt):\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    all_targets, all_preds = [], []\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels.to(device))\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_preds.append(probs)\n",
    "        all_targets.append(labels.numpy())\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    preds = np.vstack(all_preds) >= 0.5  # threshold\n",
    "    targets = np.vstack(all_targets)\n",
    "    f1 = f1_score(targets, preds, average='micro')\n",
    "    return avg_loss, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_run = ['resnet50', 'densenet121', 'efficientnet_b0']\n",
    "histories = {}\n",
    "\n",
    "for name in models_to_run:\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    model = get_model(name, len(label_cols)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    history = {'train_loss':[], 'val_loss':[], 'val_f1':[]}\n",
    "\n",
    "    for epoch in range(1, 11):   # 10 epochs\n",
    "        tl = train_one_epoch(model, train_loader, optimizer)\n",
    "        vl, vf = validate(model, val_loader)\n",
    "        history['train_loss'].append(tl)\n",
    "        history['val_loss'].append(vl)\n",
    "        history['val_f1'].append(vf)\n",
    "        print(f\"Epoch {epoch:02d}: train_loss={tl:.4f}, val_loss={vl:.4f}, val_f1={vf:.4f}\")\n",
    "\n",
    "    # save state\n",
    "    torch.save(model.state_dict(), f\"{name}_multilabel.pth\")\n",
    "    histories[name] = history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses\n",
    "plt.figure(figsize=(12,4))\n",
    "for name,h in histories.items():\n",
    "    plt.plot(h['train_loss'], label=f'{name} train')\n",
    "    plt.plot(h['val_loss'],   label=f'{name} val', linestyle='--')\n",
    "plt.title(\"Train vs Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# F1\n",
    "plt.figure(figsize=(6,4))\n",
    "for name,h in histories.items():\n",
    "    plt.plot(h['val_f1'], label=name)\n",
    "plt.title(\"Validation F1 over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Micro F1\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55812a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "for name,h in histories.items():\n",
    "    summary.append({\n",
    "        'model': name,\n",
    "        'final_val_loss': h['val_loss'][-1],\n",
    "        'best_val_f1': max(h['val_f1'])\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary).sort_values('best_val_f1', ascending=False)\n",
    "print(summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
